{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "# importing grid search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# import our models\n",
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#linear SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "#random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "import os\n",
    "#importing time to measure the time of execution\n",
    "import time\n",
    "\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>home_team</th>\n",
       "      <th>away_team</th>\n",
       "      <th>season</th>\n",
       "      <th>home_win</th>\n",
       "      <th>home_rolling_fg</th>\n",
       "      <th>home_rolling_fga</th>\n",
       "      <th>home_rolling_fg%</th>\n",
       "      <th>home_rolling_3p</th>\n",
       "      <th>home_rolling_3pa</th>\n",
       "      <th>...</th>\n",
       "      <th>away_rolling_opponent_drb%_max</th>\n",
       "      <th>away_rolling_opponent_trb%_max</th>\n",
       "      <th>away_rolling_opponent_ast%_max</th>\n",
       "      <th>away_rolling_opponent_stl%_max</th>\n",
       "      <th>away_rolling_opponent_blk%_max</th>\n",
       "      <th>away_rolling_opponent_tov%_max</th>\n",
       "      <th>away_rolling_opponent_usg%_max</th>\n",
       "      <th>away_rolling_opponent_ortg_max</th>\n",
       "      <th>away_rolling_opponent_drtg_max</th>\n",
       "      <th>away_rolling_opponent_Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-08</td>\n",
       "      <td>LAL</td>\n",
       "      <td>CHI</td>\n",
       "      <td>2020-21</td>\n",
       "      <td>1</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>0.510333</td>\n",
       "      <td>12.166667</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>37.683333</td>\n",
       "      <td>25.166667</td>\n",
       "      <td>43.016667</td>\n",
       "      <td>5.083333</td>\n",
       "      <td>8.016667</td>\n",
       "      <td>47.266667</td>\n",
       "      <td>33.933333</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>119.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-09</td>\n",
       "      <td>MIL</td>\n",
       "      <td>CLE</td>\n",
       "      <td>2020-21</td>\n",
       "      <td>1</td>\n",
       "      <td>45.166667</td>\n",
       "      <td>88.666667</td>\n",
       "      <td>0.509833</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>39.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>37.966667</td>\n",
       "      <td>25.133333</td>\n",
       "      <td>48.700000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>6.566667</td>\n",
       "      <td>56.333333</td>\n",
       "      <td>31.833333</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>109.500000</td>\n",
       "      <td>101.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-10</td>\n",
       "      <td>BRK</td>\n",
       "      <td>OKC</td>\n",
       "      <td>2020-21</td>\n",
       "      <td>0</td>\n",
       "      <td>44.666667</td>\n",
       "      <td>90.333333</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>13.166667</td>\n",
       "      <td>36.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>43.883333</td>\n",
       "      <td>25.016667</td>\n",
       "      <td>60.833333</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>5.883333</td>\n",
       "      <td>47.533333</td>\n",
       "      <td>34.033333</td>\n",
       "      <td>169.833333</td>\n",
       "      <td>113.166667</td>\n",
       "      <td>106.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-10</td>\n",
       "      <td>GSW</td>\n",
       "      <td>TOR</td>\n",
       "      <td>2020-21</td>\n",
       "      <td>1</td>\n",
       "      <td>41.333333</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>0.470833</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>44.950000</td>\n",
       "      <td>26.050000</td>\n",
       "      <td>48.866667</td>\n",
       "      <td>5.050000</td>\n",
       "      <td>12.916667</td>\n",
       "      <td>56.100000</td>\n",
       "      <td>32.033333</td>\n",
       "      <td>187.500000</td>\n",
       "      <td>123.166667</td>\n",
       "      <td>115.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-11</td>\n",
       "      <td>WAS</td>\n",
       "      <td>PHO</td>\n",
       "      <td>2020-21</td>\n",
       "      <td>1</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>91.833333</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>34.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>45.833333</td>\n",
       "      <td>25.966667</td>\n",
       "      <td>40.816667</td>\n",
       "      <td>4.550000</td>\n",
       "      <td>8.533333</td>\n",
       "      <td>38.566667</td>\n",
       "      <td>38.783333</td>\n",
       "      <td>208.333333</td>\n",
       "      <td>119.833333</td>\n",
       "      <td>108.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 269 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date home_team away_team   season  home_win  home_rolling_fg  \\\n",
       "0  2021-01-08       LAL       CHI  2020-21         1        44.000000   \n",
       "1  2021-01-09       MIL       CLE  2020-21         1        45.166667   \n",
       "2  2021-01-10       BRK       OKC  2020-21         0        44.666667   \n",
       "3  2021-01-10       GSW       TOR  2020-21         1        41.333333   \n",
       "4  2021-01-11       WAS       PHO  2020-21         1        44.000000   \n",
       "\n",
       "   home_rolling_fga  home_rolling_fg%  home_rolling_3p  home_rolling_3pa  ...  \\\n",
       "0         86.000000          0.510333        12.166667         31.500000  ...   \n",
       "1         88.666667          0.509833        16.666667         39.333333  ...   \n",
       "2         90.333333          0.493333        13.166667         36.166667  ...   \n",
       "3         87.500000          0.470833        14.000000         38.166667  ...   \n",
       "4         91.833333          0.479167        12.500000         34.833333  ...   \n",
       "\n",
       "   away_rolling_opponent_drb%_max  away_rolling_opponent_trb%_max  \\\n",
       "0                       37.683333                       25.166667   \n",
       "1                       37.966667                       25.133333   \n",
       "2                       43.883333                       25.016667   \n",
       "3                       44.950000                       26.050000   \n",
       "4                       45.833333                       25.966667   \n",
       "\n",
       "   away_rolling_opponent_ast%_max  away_rolling_opponent_stl%_max  \\\n",
       "0                       43.016667                        5.083333   \n",
       "1                       48.700000                        7.500000   \n",
       "2                       60.833333                        5.333333   \n",
       "3                       48.866667                        5.050000   \n",
       "4                       40.816667                        4.550000   \n",
       "\n",
       "   away_rolling_opponent_blk%_max  away_rolling_opponent_tov%_max  \\\n",
       "0                        8.016667                       47.266667   \n",
       "1                        6.566667                       56.333333   \n",
       "2                        5.883333                       47.533333   \n",
       "3                       12.916667                       56.100000   \n",
       "4                        8.533333                       38.566667   \n",
       "\n",
       "   away_rolling_opponent_usg%_max  away_rolling_opponent_ortg_max  \\\n",
       "0                       33.933333                      193.000000   \n",
       "1                       31.833333                      176.000000   \n",
       "2                       34.033333                      169.833333   \n",
       "3                       32.033333                      187.500000   \n",
       "4                       38.783333                      208.333333   \n",
       "\n",
       "   away_rolling_opponent_drtg_max  away_rolling_opponent_Total  \n",
       "0                      121.000000                   119.333333  \n",
       "1                      109.500000                   101.333333  \n",
       "2                      113.166667                   106.500000  \n",
       "3                      123.166667                   115.166667  \n",
       "4                      119.833333                   108.166667  \n",
       "\n",
       "[5 rows x 269 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading our data\n",
    "data_base = pd.read_pickle('data_base.pkl')\n",
    "data_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_cols = ['date',\n",
    " 'home_team',\n",
    " 'away_team',\n",
    " 'season',\n",
    " 'home_win' ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating our list of information columns. Note that these variables are all present before a game actually happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature set shape: (2027, 264), target series shape: (2027,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into train and test\n",
    "X = data_base.loc[:, ~data_base.columns.isin(info_cols)] # excluding info_cols\n",
    "y = data_base['home_win']\n",
    "\n",
    "print(f'Feature set shape: {X.shape}, target series shape: {y.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to perform Feature Scaling when we are dealing with Gradient Descent Based algorithms (Linear and Logistic Regression, Neural Network) and Distance-based algorithms (KNN, K-means, SVM) as these are very sensitive to the range of the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the data scaling and pipeline libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#Creating a class that initializes the model and the parameters, and we just need to pass the scaling method for the pipeline\n",
    "\n",
    "class ModelParams:\n",
    "        \"\"\" Info:\n",
    "         This class initializes the parameters and scalers for the models, which will be used for gridsearchcv,\n",
    "         and their scaling methods.\n",
    "          Input:\n",
    "           model: the model to be used\n",
    "            scaler: Default = False, if True, the model will be scaled accordily with its respective scaler\n",
    "          Output:\n",
    "           get_pipe: returns the model pipeline and its parameters\n",
    "           model_name: returns the model name\n",
    "             \"\"\"\n",
    "                \n",
    "        #models_list = [LogisticRegression(random_state=42), LinearSVC(random_state=42), RandomForestClassifier(random_state=42), XGBClassifier(random_state=42)]     \n",
    "    \n",
    "        #creating the models  parameters dictionary\n",
    "        # Parameters of pipelines can be set using '__' separated parameter names:\n",
    "        \n",
    "        models_params = {\n",
    "            'LogisticRegression': {'logisticregression__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "                                      'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                                        'logisticregression__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "                                          'logisticregression__max_iter': [10000]},\n",
    "            'LinearSVC': {'linearsvc__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'linearsvc__loss': ['hinge', 'squared_hinge'],\n",
    "                             'linearsvc__max_iter': [10000]},\n",
    "            'RandomForestClassifier': {'randomforestclassifier__bootstrap': [True, False],\n",
    "                                        'randomforestclassifier__n_estimators': [100, 200, 300, 400, 500, 1000, 1500],\n",
    "                                          'randomforestclassifier__criterion':['gini', 'entropy', 'log_loss'],\n",
    "                                         'randomforestclassifier__max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "                                           'randomforestclassifier__min_samples_split': [2, 5, 10],\n",
    "                                             'randomforestclassifier__min_samples_leaf': [None, 1, 2, 4],\n",
    "                                         'randomforestclassifier__max_features': ['sqrt', 'log2', 'auto'],\n",
    "                                           'randomforestclassifier__min_weight_fraction_leaf': [0.0, 0.4, None],\n",
    "                                         'randomforestclassifier__max_leaf_nodes': [None, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "                                           'randomforestclassifier__min_impurity_decrease': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 1.0]},\n",
    "            'XGBClassifier': {'xgbclassifier__n_estimators': [100, 200, 300, 400, 500], 'xgbclassifier__max_depth': [None, 5, 10, 15, 20, 25, 30],\n",
    "                                 'xgbclassifier__learning_rate': [0.001, 0.01, 0.1, 1], 'xgbclassifier__min_child_weight': [1, 3, 5],\n",
    "                                   'xgbclassifier__gamma': [0.0, 0.1, 0.2, 0.3, 0.4], 'xgbclassifier__subsample': [None, 0.6, 0.8, 1.0],\n",
    "                                     'xgbclassifier__colsample_bytree': [None, 0.6, 0.8, 1.0], 'xgbclassifier__reg_alpha': [0, 0.001, 0.005, 0.01, 0.05, 1.5]}\n",
    "        }\n",
    "\n",
    "        #creating the models respective scalers:\n",
    "        models_scalers = {\n",
    "                'LogisticRegression': StandardScaler(),\n",
    "                'LinearSVC': StandardScaler(),\n",
    "                'RandomForestClassifier': MinMaxScaler(),\n",
    "                'XGBClassifier': MinMaxScaler()\n",
    "        }\n",
    "        \n",
    "        #creating the models respective pipelines and its parameters\n",
    "        def __init__(self, model, scaler=False):\n",
    "                \"\"\" Info:\n",
    "                 This method initializes the model and the scaling method\n",
    "                  ---------------------------------------------------------------------------------------------\n",
    "\n",
    "                   Input:\n",
    "                    model: the model to be used\n",
    "                     Scaler: boolean\n",
    "                      -------------------------------------------------------------------------------------------\n",
    "                      \n",
    "                       Output:\n",
    "                        None \"\"\"\n",
    "                \n",
    "                self.model = model\n",
    "                self.scaler = scaler\n",
    "                self.model_name = model.__class__.__name__\n",
    "\n",
    "        def get_pipe(self):\n",
    "                \"\"\" Info:\n",
    "                 This method returns the model pipeline and its parameters and the model name\n",
    "                  ---------------------------------------------------------------------------------------------\n",
    "\n",
    "                   Input:\n",
    "                    None\n",
    "                    ---------------------------------------------------------------------------------------------\n",
    "\n",
    "                     Output:\n",
    "                      model pipeline, model parameters, model name \"\"\"\n",
    "                \n",
    "                self.params = self.models_params[self.model_name]\n",
    "                if self.scaler:\n",
    "                        self.scaler = self.models_scalers[self.model_name]\n",
    "                else:\n",
    "                        self.scaler = None\n",
    "                return make_pipeline(self.scaler, self.model), self.params, self.model_name\n",
    "                \n",
    "               \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "              \n",
    "      \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating a class to take in the model and return the metrics of this model with the best parameters,\n",
    "# using for this the GridSearchCV function\n",
    "class ModelDevelopment:\n",
    "    \"\"\" Takes in the model, the X and y data and splits the data into train and test sets.\n",
    "    Functions:\n",
    "    - grid_search: Takes in the parameters to be tested and the scoring metric and returns the best model, best parameters and best score\n",
    "    - model_metrics: Prints the accuracy, precision, recall, f1 and auc of best model parameters found by the grid search against the test set\n",
    "    - roc_curve: Plots the ROC curve of the best model parameters found by the grid search against the test set\"\"\"\n",
    "\n",
    "    def __init__(self, model, model_name, X, y):\n",
    "        \"\"\" Info:\n",
    "            Takes in the model, the X and y data and splits the data into train and test sets.\n",
    "\n",
    "            Input:\n",
    "            model: Model to be tested\n",
    "            model_name: Name of the model to be tested\n",
    "            X: Feature set\n",
    "            y: Target series\n",
    "            \n",
    "            \n",
    "\n",
    "            Output:\n",
    "            X_train: Feature set for the train set\n",
    "            X_test: Feature set for the test set\n",
    "            y_train: Target series for the train set\n",
    "            y_test: Target series for the test set\n",
    "            \n",
    "                    \"\"\"\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.best_model = None\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    def grid_search(self, params: dict, scoring: str): # scoring is the metric we want to optimize, mostly 'roc_auc'\n",
    "        \"\"\" Info:\n",
    "            Takes in the parameters to be tested and the scoring metric and returns the best model, best parameters and best score\n",
    "             Input:\n",
    "              params: Dictionary with the parameters to be tested\n",
    "              scoring: Metric to be optimized\n",
    "             Output:\n",
    "              best_model: Best model found by the grid search\n",
    "              best_params: Best parameters found by the grid search\n",
    "              best_score: Best score found by the grid search \"\"\"\n",
    "        \n",
    "        #measuring the time it takes to run the grid search\n",
    "        #start time\n",
    "        start = time.time()\n",
    "        \n",
    "        grid = GridSearchCV(self.model, params, cv=5, scoring=scoring, n_jobs=-1, verbose=3) #verbose = 2 so we can watch the progress in more detail\n",
    "        #trying to fit the model with the parameters\n",
    "        try:\n",
    "            grid.fit(self.X, self.y)\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", e)\n",
    "            pass\n",
    "        self.best_model = grid.best_estimator_\n",
    "        self.best_params = grid.best_params_\n",
    "        self.best_score = grid.best_score_\n",
    "\n",
    "        #final time\n",
    "        end = time.time()\n",
    "        #print the total time it took to run the grid search in minutes\n",
    "        print('Total time: ', round((end-start)/60), ' minutes')\n",
    "\n",
    "        return self.best_model, self.best_params, self.best_score\n",
    "    \n",
    "    def random_search(self, params_range: dict, scoring: str):\n",
    "        \"\"\" Info:\n",
    "            Takes in the parameters to be tested using the RandomizedSearchCV() funtion and the scoring metric and returns the best model, best parameters and best score. The randomized search allows us to test more parameters in less time.\n",
    "            --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "             Input:\n",
    "              params: Dictionary with the parameters to be tested\n",
    "              scoring: Metric to be optimized\n",
    "            --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "             Output:\n",
    "              best_model: Best model found by the grid search\n",
    "              best_params: Best parameters found by the grid search\n",
    "              best_score: Best score found by the grid search \"\"\"\n",
    "        \n",
    "        #measuring the time it takes to run the grid search\n",
    "        #start time\n",
    "        start = time.time()\n",
    "        \n",
    "        grid = RandomizedSearchCV(self.model, params_range, cv=3, scoring=scoring, n_jobs=-1, verbose=3, n_iter=1000 random_state=42)\n",
    "        #trying to fit the model with the parameters\n",
    "        try:\n",
    "            grid.fit(self.X, self.y)\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", e)\n",
    "            pass\n",
    "        self.best_model = grid.best_estimator_\n",
    "        self.best_params = grid.best_params_\n",
    "        self.best_score = grid.best_score_\n",
    "        #final time\n",
    "        end = time.time()\n",
    "        #print the total time it took to run the grid search in minutes\n",
    "        print('Total time: ', round((end-start)/60), ' minutes')\n",
    "        \n",
    "        \n",
    "    \n",
    "    def model_metrics(self):\n",
    "        \"\"\" Prints the accuracy, precision, recall, f1 and auc of best model parameters found by the grid search against the test set\n",
    "            For this, we use the pre split X test and y test data, which are 20% of the original data\n",
    "          \"\"\"\n",
    "        y_pred = self.best_model.predict(self.X_test) #type: ignore\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        precision = precision_score(self.y_test, y_pred)\n",
    "        recall = recall_score(self.y_test, y_pred)\n",
    "        f1 = f1_score(self.y_test, y_pred)\n",
    "        auc = roc_auc_score(self.y_test, y_pred)\n",
    "        print(f'Accuracy: {accuracy:.1%}' )\n",
    "        print(f'Precision: {precision:.1%}')\n",
    "        print(f'Recall: {recall:.1%}')\n",
    "        print(f'F1: {f1:.1%}')\n",
    "        print(f'AUC: {auc:.1%}') #auc score\n",
    "        \n",
    "    \n",
    "    def roc_curve(self):\n",
    "        \"\"\" Plots the ROC curve of the best model parameters found by the grid search against the test set\n",
    "            For this, we use the pre split X test and y test data, which are 20% of the original data\n",
    "          \"\"\"\n",
    "        y_pred_proba = self.best_model.predict_proba(self.X_test)[:,1] #type: ignore\n",
    "        fpr, tpr, thresholds = roc_curve(self.y_test, y_pred_proba)\n",
    "        plt.plot([0,1], [0,1], 'k--')\n",
    "        plt.plot(fpr, tpr, label=self.model_name)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(self.model_name+' ROC Curve')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function that uses the ModelParams class to instantiates the model, its parameters and its respective scaler. Then, it instantiates the ModelDevelopment class and \n",
    "#uses the grid_search function to find the best parameters for the model.\n",
    "def instantiate_best_model(model, X, y, scoring, scaler=True, random_search=False):\n",
    "        \"\"\" Info:\n",
    "                returns the model tunned with the best parameters\n",
    "            -------------------------------------------------\n",
    "             Input:\n",
    "                model: Model to be tested\n",
    "                X: Feature set\n",
    "                y: Target series\n",
    "                scoring: Metric to be optimized\n",
    "                scaler: Whether to use a scaler or not\n",
    "             ------------------------------------------------\n",
    "             Output:\n",
    "               best_model: Best model found by the grid search\n",
    "               best_params: Best parameters found by the grid search\n",
    "               best_score: Best score found by the grid search \"\"\"\n",
    "        \n",
    "        # Using the ModelParams class to instantiate the model, its parameters and its respective scaler\n",
    "        model_params = ModelParams(model, scaler)\n",
    "        pipe, params, model_name = model_params.get_pipe()\n",
    "        #checking if the model is already saved in the models folder\n",
    "        # if the model is not saved, we instantiate the ModelDevelopment class and use the grid_search function to find the best parameters for the model and then save it\n",
    "        if scaler:\n",
    "                if not os.path.exists('models/'+model_name+'.pkl'): \n",
    "                         # Using the ModelDevelopment class to instantiate the model\n",
    "                        clf_instance = ModelDevelopment(pipe, model_name, X, y)\n",
    "                        # Using the search function to find the best parameters for the model\n",
    "                        if random_search:\n",
    "                                best_model, best_params, best_score  = clf_instance.random_search(params, scoring)\n",
    "                        else:\n",
    "                                best_model, best_params, best_score  = clf_instance.grid_search(params, scoring) # This also fits the model instance with a variety of parameters against the dataset\n",
    "                        #saving best model with best parameters to pickle file:\n",
    "                        file_name = 'models/'+model_name+'.pkl'\n",
    "                        instance_name = 'models/'+model_name+'_instance.pkl'\n",
    "                        with open(file_name, 'wb') as f:\n",
    "                                pickle.dump(best_model, f)\n",
    "                        with open(instance_name, 'wb') as f:\n",
    "                                pickle.dump(clf_instance, f) \n",
    "                else:\n",
    "                        pass\n",
    "                       \n",
    "        else:\n",
    "                if not os.path.exists('models/'+model_name+'_unscaled.pkl'):\n",
    "                         # Using the ModelDevelopment class to instantiate the model\n",
    "                        clf_instance = ModelDevelopment(pipe, model_name, X, y)\n",
    "                        # Using the grid_search function to find the best parameters for the model\n",
    "                        best_model, *_  = clf_instance.grid_search(params, scoring) # This also fits the model instance with a variety of parameters against the dataset\n",
    "                        #saving best model with best parameters to pickle file:\n",
    "                        file_name = 'models/'+model_name+'_unscaled.pkl'\n",
    "                        instance_name = 'models/'+model_name+'_unscaled_instance.pkl'\n",
    "                        with open(file_name, 'wb') as f:\n",
    "                                pickle.dump(best_model, f)\n",
    "                        with open(instance_name, 'wb') as f:\n",
    "                                pickle.dump(clf_instance, f) \n",
    "                else:\n",
    "                        pass     \n",
    "                                     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our best models objects with scaled data and saving it with pickle file, With all the propertys included with the class we created, model develpment.\n",
    "\n",
    "### Because emsemble models will have naturally more parameters, and are more complex, we are going to set our search method to random search on these models\n",
    "\n",
    "For this, we are going to create a function that takes in a model, the respective scaler. We are going to train either with the scaler on and off.\n",
    "\n",
    "models_list = [LogisticRegression(random_state=42), LinearSVC(random_state=42), RandomForestClassifier(random_state=42), XGBClassifier(random_state=42)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the best Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "240 fits failed out of a total of 450.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.5               nan 0.5        0.8217766\n",
      " 0.8217766  0.82174657 0.82178154 0.8217766         nan        nan\n",
      "        nan        nan        nan        nan        nan 0.81906825\n",
      "        nan 0.81902989 0.81536094 0.81536585 0.81542009 0.81536585\n",
      " 0.81536097        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.82064031        nan 0.8207095  0.79866077\n",
      " 0.79865087 0.79861153 0.79866075 0.79866566        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.79437956\n",
      "        nan 0.79441391 0.78695126 0.78692662 0.78693164 0.78687225\n",
      " 0.78690193        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.77730774        nan 0.77806673 0.77749455\n",
      " 0.77750926 0.7774945  0.77770108 0.7781153         nan        nan\n",
      "        nan        nan        nan        nan        nan 0.77127137\n",
      "        nan 0.77303974 0.77180872 0.77185826 0.77181366 0.77241915\n",
      " 0.7733503         nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time:  2  minutes\n"
     ]
    }
   ],
   "source": [
    "instantiate_best_model(LogisticRegression(random_state=42), X, y, 'roc_auc', scaler=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.8%\n",
      "Precision: 77.3%\n",
      "Recall: 82.7%\n",
      "F1: 79.9%\n",
      "AUC: 76.1%\n"
     ]
    }
   ],
   "source": [
    "LR_instance = pickle.load(open('models/LogisticRegression_instance.pkl', 'rb'))\n",
    "LR_instance.model_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logisticregression__C': 0.001,\n",
       " 'logisticregression__max_iter': 10000,\n",
       " 'logisticregression__penalty': 'l2',\n",
       " 'logisticregression__solver': 'sag'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_instance.best_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the best SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "Total time:  1  minutes\n"
     ]
    }
   ],
   "source": [
    "instantiate_best_model(LinearSVC(random_state=42), X, y, 'roc_auc', scaler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.1%\n",
      "Precision: 79.3%\n",
      "Recall: 79.6%\n",
      "F1: 79.5%\n",
      "AUC: 76.8%\n"
     ]
    }
   ],
   "source": [
    "SVM_instance = pickle.load(open('models/LinearSVC_instance.pkl', 'rb'))\n",
    "SVM_instance.model_metrics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the best RandomForest model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instantiate_best_model(RandomForestClassifier(random_state=42), X, y, 'roc_auc', scaler=True, random_search=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_instance = pickle.load(open('models/RandomForestClassifier_instance.pkl', 'rb'))\n",
    "forest_instance.model_metrics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the best XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instantiate_best_model(XGBClassifier(random_state=42), X, y, 'roc_auc', scaler=True, random_search=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_instance = pickle.load(open('models/XGBClassifier_instance.pkl', 'rb'))\n",
    "xgb_instance.model_metrics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unscaled data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "240 fits failed out of a total of 450.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.78976474        nan 0.78933647 0.81951801\n",
      " 0.81949791 0.81943376 0.81951785 0.81955738        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.82365953\n",
      "        nan 0.82392096 0.8114614  0.811476   0.81150563 0.81194006\n",
      " 0.81222105        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.81534747        nan 0.81620078 0.80072756\n",
      " 0.80049557 0.80074232 0.80440658 0.80671018        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.79828563\n",
      "        nan 0.8071982  0.79128406 0.79102394 0.7912747  0.80232061\n",
      " 0.80569934        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.78422012        nan 0.80571437 0.78275948\n",
      " 0.78388746 0.78291705 0.80209394 0.805537          nan        nan\n",
      "        nan        nan        nan        nan        nan 0.77819355\n",
      "        nan 0.80557149 0.77751763 0.78043061 0.77782902 0.80209396\n",
      " 0.80555174        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time:  14  minutes\n"
     ]
    }
   ],
   "source": [
    "instantiate_best_model(LogisticRegression(random_state=42), X, y, 'roc_auc', scaler=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.4%\n",
      "Precision: 76.2%\n",
      "Recall: 81.0%\n",
      "F1: 78.5%\n",
      "AUC: 74.7%\n"
     ]
    }
   ],
   "source": [
    "LR_instance_unscaled = pickle.load(open('models/LogisticRegression_unscaled_instance.pkl', 'rb'))\n",
    "LR_instance_unscaled.model_metrics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best svm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "Total time:  3  minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joaot\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "instantiate_best_model(LinearSVC(random_state=42), X, y, 'roc_auc', scaler=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.1%\n",
      "Precision: 79.4%\n",
      "Recall: 81.9%\n",
      "F1: 80.6%\n",
      "AUC: 77.6%\n"
     ]
    }
   ],
   "source": [
    "svm_instance_unscaled = pickle.load(open('models/LinearSVC_unscaled_instance.pkl', 'rb'))\n",
    "svm_instance_unscaled.model_metrics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instantiate_best_model(RandomForestClassifier(random_state=42), X, y, 'roc_auc', scaler=False, random_search=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_instance_unscaled = pickle.load(open('models/RandomForestClassifier_unscaled_instance.pkl', 'rb'))\n",
    "forest_instance_unscaled.model_metrics()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instantiate_best_model(XGBClassifier(random_state=42), X, y, 'roc_auc', scaler=False, random_search=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_instance_unscaled = pickle.load(open('models/XGBClassifier_unscaled_instance.pkl', 'rb'))\n",
    "xgb_instance_unscaled.model_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2af2c5b7bb1193408b7423a70bbd1c8b2e4337f6ccafc2523e8be7b83a1592e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
